{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate\n",
    "!pip install datasets\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "model_id = \"google/gemma-1.1-2b-it\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"Kubermatic/cncf-raw-data-for-llm-training\", split=\"train\")\n",
    "\n",
    "#taken in parts from https://medium.com/@lucamassaron/sherlock-holmes-q-a-enhanced-with-gemma-2b-it-fine-tuning-2907b06d2645\n",
    "\n",
    "def gemma_result(question, model=model, tokenizer=tokenizer, temperature=0.0, max_new_tokens = 256, return_answer=False):\n",
    "    input_ids = tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
    "    if temperature > 0:\n",
    "        do_sample=True\n",
    "        outputs = model.generate(**input_ids,\n",
    "                                max_new_tokens=max_new_tokens,\n",
    "                                do_sample=do_sample,\n",
    "                                temperature=temperature)\n",
    "    else:\n",
    "        do_sample=False\n",
    "        outputs = model.generate(**input_ids,\n",
    "                                max_new_tokens=max_new_tokens)\n",
    "    result = str(tokenizer.decode(outputs[0])).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").strip()\n",
    "    if return_answer:\n",
    "        return result\n",
    "    else:\n",
    "        print(result)\n",
    "\n",
    "qa_data = []\n",
    "fail_count = 0\n",
    "\n",
    "\n",
    "def extract_json(text, word):\n",
    "    pattern = fr'\"{word}\": \"(.*?)\"'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"\"\n",
    "# chunks = 2 # increment this number up to len(extracted_texts)\n",
    "\n",
    "question = \"\"\n",
    "answer = \"\"\n",
    "question_ratio = 1000 # decrement this number to produce more questions (suggested: 24)\n",
    "\n",
    "for i in tqdm(range(len(dataset['content']))):\n",
    "    text_category = dataset['tag'][i]['category']\n",
    "    text_subcategory = dataset['tag'][i]['subcategory']\n",
    "    text_project = dataset['tag'][i]['project_name']\n",
    "    information_chunk = dataset['content'][i][0]['data']\n",
    "\n",
    "    question_text = f\"\"\"Create a question and its answer from the following piece of information for a project of the Cloud Native Computing Foundation landscape,\n",
    "    do not assume the reader knows the text hence put all the necessary information into the question,\n",
    "    and return it exclusively in JSON format in the format {'{\"question\": \"...\", \"answer\": \"...\"}'}.\n",
    "    Here is the piece of information to elaborate: \n",
    "    \"{information_chunk}\"\n",
    "\n",
    "    OUTPUT JSON:\n",
    "    \"\"\"\n",
    "    # no_questions = max(1, (len(dataset['content'][i][0]['data']) // question_ratio))\n",
    "    no_questions = 1\n",
    "    for j in range(no_questions):\n",
    "      try:\n",
    "        result = gemma_result(question_text, model=model, temperature=0, return_answer=True)\n",
    "        result = result.split(\"OUTPUT JSON:\")[-1]\n",
    "      \n",
    "        question = extract_json(result, \"question\")\n",
    "        answer = extract_json(result, \"answer\")\n",
    "        text_project = dataset['tag'][i]['project_name']\n",
    "        qa_data.append([f\"{question}\",f\"{answer}\",f\"{text_project}\"])\n",
    "      except:\n",
    "        print(f\"Gemma wasn't able to create a proper question answer pair. No. of failed attempts: {fail_count}\") \n",
    "        fail_count =+ 1\n",
    "# opening the csv file in 'a+' mode\n",
    "file = open('/content/gdrive/My Drive/filename.csv', 'a+', newline ='')\n",
    " \n",
    "# writing the data into the file\n",
    "with file:    \n",
    "    write = csv.writer(file)\n",
    "    write.writerows(qa_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
