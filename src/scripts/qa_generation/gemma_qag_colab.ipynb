{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script installs necessary packages, logs into Hugging Face Hub, and sets up a language model for generating question-answer pairs using Gemma-2B-IT.\n",
    "It fetches a dataset from Kubermatic for training, and employs a function 'gemma_result' to generate QA pairs from extracted text snippets, saving results in a CSV file.\n",
    "\n",
    "Dependencies:\n",
    "- accelerate\n",
    "- datasets\n",
    "- huggingface_hub\n",
    "- transformers\n",
    "- torch\n",
    "- tqdm\n",
    "- csv\n",
    "- google.colab\n",
    "\n",
    "Usage:\n",
    "Ensure correct mounting of Google Drive for output. Adjust 'question_ratio' to control QA generation frequency.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "!pip install accelerate\n",
    "!pip install datasets\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "model_id = \"google/gemma-1.1-2b-it\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"Kubermatic/cncf-raw-data-for-llm-training\", split=\"train\")\n",
    "\n",
    "#taken in parts from https://medium.com/@lucamassaron/sherlock-holmes-q-a-enhanced-with-gemma-2b-it-fine-tuning-2907b06d2645\n",
    "\n",
    "def gemma_result(question: str, \n",
    "                 model: torch.nn.Module = model, \n",
    "                 tokenizer = tokenizer, \n",
    "                 temperature: float = 0.0, \n",
    "                 max_new_tokens: int = 256, \n",
    "                 return_answer: bool = False) -> None:\n",
    "                 \n",
    "    \"\"\"\n",
    "    Generate a response to a given question using a model.\n",
    "\n",
    "    Args:\n",
    "        question (str): The input question to generate a response for.\n",
    "        model (torch.nn.Module): The pretrained model to use for generating responses.\n",
    "        tokenizer: The tokenizer used for tokenizing the input.\n",
    "        temperature (float, optional): The temperature parameter for sampling. Default is 0.0.\n",
    "        max_new_tokens (int, optional): The maximum number of tokens to generate. Default is 256.\n",
    "        return_answer (bool, optional): Whether to return the answer instead of printing it. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        None if return_answer is False, otherwise returns the generated answer as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
    "    if temperature > 0:\n",
    "        do_sample=True\n",
    "        outputs = model.generate(**input_ids,\n",
    "                                max_new_tokens=max_new_tokens,\n",
    "                                do_sample=do_sample,\n",
    "                                temperature=temperature)\n",
    "    else:\n",
    "        do_sample=False\n",
    "        outputs = model.generate(**input_ids,\n",
    "                                max_new_tokens=max_new_tokens)\n",
    "    result = str(tokenizer.decode(outputs[0])).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").strip()\n",
    "    if return_answer:\n",
    "        return result\n",
    "    else:\n",
    "        print(result)\n",
    "\n",
    "qa_data = []\n",
    "fail_count = 0\n",
    "\n",
    "\n",
    "def extract_json(text: str, word: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the value associated with a specified key from JSON-formatted text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The JSON-formatted text string.\n",
    "        word (str): The key to extract the value for.\n",
    "\n",
    "    Returns:\n",
    "        str: The value associated with the key 'word' in the JSON text, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    pattern = fr'\"{word}\": \"(.*?)\"'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"\"\n",
    "# chunks = 2 # increment this number up to len(extracted_texts)\n",
    "\n",
    "question = \"\"\n",
    "answer = \"\"\n",
    "question_ratio = 1000 # decrement this number to produce more questions (suggested: 24)\n",
    "\n",
    "for i in tqdm(range(len(dataset['content']))):\n",
    "    text_category = dataset['tag'][i]['category']\n",
    "    text_subcategory = dataset['tag'][i]['subcategory']\n",
    "    text_project = dataset['tag'][i]['project_name']\n",
    "    information_chunk = dataset['content'][i][0]['data']\n",
    "\n",
    "    question_text = f\"\"\"Create a question and its answer from the following piece of information for a project of the Cloud Native Computing Foundation landscape,\n",
    "    do not assume the reader knows the text hence put all the necessary information into the question,\n",
    "    and return it exclusively in JSON format in the format {'{\"question\": \"...\", \"answer\": \"...\"}'}.\n",
    "    Here is the piece of information to elaborate: \n",
    "    \"{information_chunk}\"\n",
    "\n",
    "    OUTPUT JSON:\n",
    "    \"\"\"\n",
    "    # no_questions = max(1, (len(dataset['content'][i][0]['data']) // question_ratio))\n",
    "    no_questions = 1\n",
    "    for j in range(no_questions):\n",
    "      try:\n",
    "        result = gemma_result(question_text, model=model, temperature=0, return_answer=True)\n",
    "        result = result.split(\"OUTPUT JSON:\")[-1]\n",
    "      \n",
    "        question = extract_json(result, \"question\")\n",
    "        answer = extract_json(result, \"answer\")\n",
    "        text_project = dataset['tag'][i]['project_name']\n",
    "        qa_data.append([f\"{question}\",f\"{answer}\",f\"{text_project}\"])\n",
    "      except:\n",
    "        print(f\"Gemma wasn't able to create a proper question answer pair. No. of failed attempts: {fail_count}\") \n",
    "        fail_count =+ 1\n",
    "# opening the csv file in 'a+' mode\n",
    "file = open('/content/gdrive/My Drive/filename.csv', 'a+', newline ='')\n",
    " \n",
    "# writing the data into the file\n",
    "with file:    \n",
    "    write = csv.writer(file)\n",
    "    write.writerows(qa_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
