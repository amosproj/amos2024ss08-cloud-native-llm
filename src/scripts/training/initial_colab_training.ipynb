{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U trl\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "import bitsandbytes as bnb\n",
    "import accelerate\n",
    "from trl import SFTTrainer\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoModelForQuestionAnswering,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments,\n",
    "                          )\n",
    "\n",
    "\n",
    "# training pipeline taken from https://huggingface.co/blog/gemma-peft\n",
    "model_id = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# TODO: Check if this can be changed to AutoModelForQuestionAnswering with GEMMA\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "# Training Data\n",
    "dataset = load_dataset(\"Kubermatic/cncf-question-and-answer-dataset-for-llm-training\", split=\"train[:50]\")\n",
    "\n",
    "\n",
    "# Training (hyper)parameters (initial config taken from: https://medium.com/@lucamassaron/sherlock-holmes-q-a-enhanced-with-gemma-2b-it-fine-tuning-2907b06d2645)\n",
    "max_seq_length = 1024\n",
    "\n",
    "\n",
    "output_dir = \"trained_model\"\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=False,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 500,\n",
    "    eval_accumulation_steps=1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    disable_tqdm=True,\n",
    "    # debug=\"underflow_overflow\"\n",
    ")\n",
    "\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['Question'][0]}\\nAuthor: {example['Answer'][0]}<eos>\"\n",
    "    return [text]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    # TODO: Check if this can be changed to QUESTION_ANS with GEMMA\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_arguments,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
