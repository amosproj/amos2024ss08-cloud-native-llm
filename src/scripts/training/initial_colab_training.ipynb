{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script installs and imports necessary packages, logs into Hugging Face Hub, and sets up a training pipeline for a language model using Gemma-1.1-2B-IT. It fine-tunes the model on a specific dataset, configuring hyperparameters and using advanced training techniques.\n",
    "\n",
    "Dependencies:\n",
    "- accelerate\n",
    "- datasets\n",
    "- trl\n",
    "- peft\n",
    "- bitsandbytes\n",
    "- huggingface_hub\n",
    "- transformers\n",
    "- torch\n",
    "- tqdm\n",
    "- pandas\n",
    "\n",
    "Environment Setup:\n",
    "Ensure 'GITHUB_TOKEN' environment variable is configured for Hugging Face Hub authentication.\n",
    "\n",
    "Training Data:\n",
    "- Uses dataset from \"Kubermatic/cncf-question-and-answer-dataset-for-llm-training\".\n",
    "\n",
    "Training Pipeline:\n",
    "- Utilizes 'SFTTrainer' from 'trl' with 'LoraConfig' for training configuration.\n",
    "- Fine-tunes 'AutoModelForCausalLM' with custom configurations including 4-bit quantization.\n",
    "\n",
    "Output:\n",
    "- Trained model and tokenizer are saved to 'trained_model'.\n",
    "\"\"\"\n",
    "\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U trl\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "import bitsandbytes as bnb\n",
    "import accelerate\n",
    "from trl import SFTTrainer\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoModelForQuestionAnswering,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments,\n",
    "                          )\n",
    "\n",
    "\n",
    "# training pipeline taken from https://huggingface.co/blog/gemma-peft\n",
    "model_id = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# TODO: Check if this can be changed to AutoModelForQuestionAnswering with GEMMA\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "# Training Data\n",
    "dataset = load_dataset(\"Kubermatic/cncf-question-and-answer-dataset-for-llm-training\", split=\"train[:50]\")\n",
    "\n",
    "\n",
    "# Training (hyper)parameters (initial config taken from: https://medium.com/@lucamassaron/sherlock-holmes-q-a-enhanced-with-gemma-2b-it-fine-tuning-2907b06d2645)\n",
    "max_seq_length = 1024\n",
    "\n",
    "\n",
    "output_dir = \"trained_model\"\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=False,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 500,\n",
    "    eval_accumulation_steps=1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    disable_tqdm=True,\n",
    "    # debug=\"underflow_overflow\"\n",
    ")\n",
    "\n",
    "\n",
    "def formatting_func(example):\n",
    "    \"\"\"\n",
    "    Formats a dictionary containing 'Question' and 'Answer' keys into a specific text format.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A dictionary containing 'Question' and 'Answer' keys.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing formatted strings based on the input example.\n",
    "    \"\"\"\n",
    "    text = f\"### Question: {example['Question'][0]}\\nAuthor: {example['Answer'][0]}<eos>\"\n",
    "    return [text]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    # TODO: Check if this can be changed to QUESTION_ANS with GEMMA\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_arguments,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
