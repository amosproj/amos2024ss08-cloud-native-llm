name: gemma
context_size: 512
f16: true
threads: 14
gpu_layers: 90
mmap: true
parameters:
  # Reference any HF model or a local file here
  #model: huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf
  model: huggingface://google/gemma-2b-it/gemma-2b-it.gguf
  temperature: 0.7
  top_k: 50
  top_p: 0.95

backend: langchain-huggingface

template:

  chat: &template |
    Instruct: {{.Input}}
    Output:
  # Modify the prompt template here ^^^ as per your requirements
  completion: *template